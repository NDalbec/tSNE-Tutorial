{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Under the Hood of t-SNE\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/under_the_hood.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "The goal of this tutorial is to provide a general understanding of *t-Distributed Stochastic Neighbor Embedding* (t-SNE).\n",
    "The notebook follows the development described in the original t-SNE paper :\n",
    "\n",
    "> [1] L. Van der Maaten and G. Hinton, ‘Visualizing data using t-SNE.’, \n",
    ">    Journal of machine learning research, vol. 9, no. 11, 2008, \n",
    ">    Accessed: Apr. 03, 2024. [Online]. \n",
    ">    Available: https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl\n",
    "\n",
    "The visuals are heavily inspired by multiple sources [2,3] and the code is a simplified version of the scikit-learn implementation [4] to understand the main idea behind t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "* 1. [Problem Formulation](#problemformulation)\n",
    "* 2. [First let's understand Stochastic Neighbor Embedding (SNE)](#sNE)\n",
    "    * 2.1 [Visual Representation of Similarities](#visual_similarity)\n",
    "    * 2.2 [Mathematical Representation of Similarities](#math_similarity)\n",
    "* 3. [What are we optimizing?](#optimization)\n",
    "    * 3.1 [Cost Function C](#cost_c)\n",
    "    * 3.2 [Gradient Descent to Optimize $Y = \\{y_1, y_2, ..., y_n\\}$](#gradient_descent)\n",
    "* 4. [What About $\\sigma_i$?](#sigma)\n",
    "    * 4.1 [How should we select this parameter?](#sigma_selection)\n",
    "    * 4.2 [Perplexity](#perplexity)\n",
    "    * 4.3 [Perplexity of a Probability Model](#perplexity_def)\n",
    "    * 4.4 [Intuition for Perplexity](#perplexity_intuition)\n",
    "    * 4.5 [Binary search for $\\sigma_i$](#binary_search)\n",
    "* 6. [t-Distributed Stochastic Neighbor Embedding](#tsne)\n",
    "    * 6.1 [Symmetric SNE](#sym_tsne)\n",
    "    * 6.2 [Pairwise Similarites](#paiwise_distance)\n",
    "    * 6.3 [Crowding Problem](#crowding)\n",
    "    * 6.4 [Mismatched Tails can Compensate for Mismatched Dimensionalities](#mismatched_tails)\n",
    "* 7. [Final Algorithm](#final_alg)\n",
    "* 8. [Now let's Implement t-SNE](#implement)\n",
    "    * 8.1 [Random initialization of the low-dimensional representation](#init)\n",
    "    * 8.2 [Matrix of euclidian distances](#distance)\n",
    "    * 8.3 [Joint probabilities to represent Similarities (high-dimensional)](#joint_prob)\n",
    "    * 8.4 [Gradient of the KL divergence](#gradient)\n",
    "    * 8.5 [Gradient descent](#gradient_descent_code)\n",
    "    * 8.6 [Compile everything in one class](#compile)\n",
    "    * 8.7 [Complete code](#complete_code)\n",
    "    * 8.8 [Let's test the described implementation](#test_implementation)\n",
    "* 9. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Formulation <a class=\"anchor\" id=\"problemformulation\"></a>\n",
    "\n",
    "t-SNE is a dimensionality reduction (DR) technique. This process attempts to reduce the number of dimensions associated to a dataset and retain a maximum amount of information. In this tutorial DR offers a way to transform a data matrix $X\\in{\\rm I\\!R}^{p\\times q}$ containing $p$ signals of size $q$ into a low-dimensionality representation $Y\\in {\\rm I\\!R}^{p\\times l}$ ($l<q$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First let's understand Stochastic Neighbor Embedding (SNE) <a class=\"anchor\" id=\"sNE\"></a>\n",
    "\n",
    "- 'Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between datapoints into conditional probabilities that represent similarities.' [1]\n",
    "\n",
    "- 'Conditional probability, $p_{j|i}$, that $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$.' [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visual Representation of Similarities <a class=\"anchor\" id=\"visual_similarity\"></a>\n",
    "\n",
    "Imagine a 2D toy dataset with 8 points. 4 points come from a Gaussian centered in (0,0) and 4 from a Gaussian centered in (0,1).\n",
    "Let's evaluate the similarities for a single point $x_i$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/initial_distribution_focus.svg\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "The distance is converted in a conditional probability. In the first dimension the similarity between points $x_i$ and $x_j$ resembles a simple 1D Gaussian.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/gaussian.svg\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Representation of Similarities <a class=\"anchor\" id=\"math_similarity\"></a>\n",
    "\n",
    "$$\n",
    "\\text{Conditional probability (high-dimensional) : }\\quad\\quad p_{j|i} = \\frac{\\exp\\left( -||x_i - x_j||^2 / 2\\sigma_i^2\\right)}{\\sum_{k\\neq i}\\exp\\left( -||x_i - x_k||^2 / 2\\sigma_i^2\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Conditional probability (low-dimensional) : }\\quad\\quad q_{j|i} = \\frac{\\exp\\left( -||y_i - y_j||^2 \\right)}{\\sum_{k\\neq i}\\exp\\left( -||y_i - y_k||^2 \\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "- $p_{j|i}$ : Similarity between high-dimensional points $x_i$ and $x_j$.\n",
    "- $q_{j|i}$ : Similarity between low-dimensional points $y_i$ and $y_j$.\n",
    "- $x_i$ : High-dimensional samples\n",
    "- $y_i$ : Low-dimensional samples\n",
    "- $\\sigma_i$ : Standard deviation of the Gaussians in high-dimensional space (set to $1/\\sqrt{2}$ for low-dimensional space)\n",
    "\n",
    "\n",
    "Note : $p_{i|i} = 0$ and $q_{i|i} = 0$, since the focus is pairwise similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we optimizing? <a class=\"anchor\" id=\"optimization\"></a>\n",
    "\n",
    "Typically dimensionality reduction techniques can be formulated in terms of an optimization problem. The question becomes: What process can use the initial data $X$ to obtain this ideal low-dimensional representation $Y$? For SNE we 'minimize the sum of the Kullback-Leibler divergences (also called relative entropy) over all data points using a gradient descent'[1]. This metric is a 'statistical distance that measure of how one probability distribution $P$ is different from a second reference probability distribution $Q$'[5].\n",
    "\n",
    "### Cost Function C <a class=\"anchor\" id=\"cost_c\"></a>\n",
    "\n",
    "Formally the cost function is defined as :\n",
    "\n",
    "$$\n",
    "C = \\sum_i KL(P_i||Q_i) = \\sum_i \\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}}\n",
    "$$\n",
    "\n",
    "### Gradient Descent to Optimize $Y = \\{y_1, y_2, ..., y_n\\}$ <a class=\"anchor\" id=\"gradient_descent\"></a>\n",
    "\n",
    "The gradient descent optimizes the cost function using the following iterative step. $Y$ can be initialized using an arbitrary process like random points or principal component analysis (PCA).\n",
    "\n",
    "$$\n",
    "Y^{(t)} = Y^{(t-1)} + \\eta \\frac{\\partial C}{\\partial y_i} +\\alpha(t) (Y^{(t-1)} - Y^{(t-2)})\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y_i} = 2 \\sum_j (p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j})(y_i - y_j)\n",
    "$$\n",
    "\n",
    "\n",
    "- $Y^{(t)}$ : Solution at iteration $t$.\n",
    "- $\\eta$ : Learning rate\n",
    "- $\\alpha(t)$ : Momentum at iteration $t$\n",
    "- $\\frac{\\partial C}{\\partial y_i}$ : Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What About $\\sigma_i$? <a class=\"anchor\" id=\"sigma\"></a>\n",
    "\n",
    "One important parameter missing from the method previously described is the variance $\\sigma_i^2$ used to evaluate the similarities of every high-dimensional point $x_i$. In denser regions it would be interesting to have smaller values for $\\sigma_i^2$ to capture important features of the data. \n",
    "\n",
    "### How should we select this parameter? <a class=\"anchor\" id=\"sigma_selection\"></a>\n",
    "\n",
    "An interesting way to think of the problem is by looking at the entropy associated to the similarity functions (Gaussians), the entropy increases as $\\sigma_i$ increases. But is it actually easier for humans to approximate the entropy of our high-dimensional data instead of $\\sigma_i$? \n",
    "\n",
    "Some people might think entropy is better since it is a measure of uncertainty and it is possible to think of it as the number of bits needed to encode random samples from a distribution [6]. With some effort we could estimate this entropy, although I personally find it difficult to have an intuition for this value.\n",
    "\n",
    "<details>\n",
    "  <summary><i>Proof of Entropy increasing </i></summary>\n",
    "\n",
    "   \\begin{aligned}\n",
    "    H(x) & =-\\int p(x) \\log p(x) \\mathrm{d} x \\\\\n",
    "    & =-\\mathbb{E}\\left[\\log \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\right] \\\\\n",
    "    & =-\\mathbb{E}\\left[\\log \\left[\\left(2 \\pi \\sigma^2\\right)^{-1 / 2} \\exp \\left(-\\frac{1}{2 \\sigma^2}(x-\\mu)^2\\right)\\right]\\right] \\\\\n",
    "    & =\\frac{1}{2} \\log \\left(2 \\pi \\sigma^2\\right)+\\frac{1}{2 \\sigma^2} \\mathbb{E}\\left[(x-\\mu)^2\\right] \\\\\n",
    "    & = \\frac{1}{2} \\log \\left(2 \\pi \\sigma^2\\right)+\\frac{1}{2} .\n",
    "    \\end{aligned}\n",
    "   \n",
    "   From [7]\n",
    "    \n",
    "</details>\n",
    "\n",
    "This is where perplexity comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perplexity <a class=\"anchor\" id=\"perplexity\"></a>\n",
    "\n",
    "We use instead Perplexity which is defined as\n",
    "\n",
    "$$\n",
    "\\text{Perplexity :}\\quad\\quad Perp(P_i) = 2^{H(P_i)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Shannon entropy :}\\quad\\quad H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i} \n",
    "$$\n",
    "\n",
    "### Perplexity of a Probability Model <a class=\"anchor\" id=\"perplexity_def\"></a>\n",
    "\n",
    "If we look at the Wikipedia definition : \n",
    "\n",
    "'In information theory, perplexity is a measure of uncertainty in the value of a sample from a discrete probability distribution. The larger the perplexity, the less likely it is that an observer can guess the value which will be drawn from the distribution.' [8]\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary><i>Why does Perplexity look so familiar? </i></summary>\n",
    "\n",
    "   Recall your Quantum information/Classical information class where we discuss Shannon's noiseless coding theorem. We typically discuss that for a string of $N$ bits we can encode $2^N$ messages (combinations). Although, given messages follow specific probability distributions, it is possible to reduce on average the number of bits needed to encode messages. It follows that the number of different typical messages that can be sent is approximately :  \n",
    "\n",
    "   $$\n",
    "    W = 2^{NH(p)}\n",
    "   $$\n",
    "\n",
    "  'Hence it is only necessary to use $NH(p)$ bits rather than $N$ bits to faithfully encode the message.' [9]\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuition for Perplexity <a class=\"anchor\" id=\"perplexity_intuition\"></a>\n",
    "\n",
    "In the original paper for t-SNE it is explained that, 'the perplexity can be interpreted as a smooth measure of the effective number of neighbors' [1]. This is why the user defined parameter for SNE (and t-SNE) is the perplexity and not $\\sigma_i$. On a graph we can see that increasing $\\sigma_i$ (increasing the perplexity) increases the number of significant neighbors (effective neighbors). On the following figure, we can see how for $\\sigma_i=0.1$ we have effectively zero points similar to $x_i$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/gaussian_2sigma.svg\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "Also, an interesting fact about perplexity is that for a uniform discrete variable with $K$ outcomes, the perplexity is directly $K$.\n",
    "Coming back to the selection of $\\sigma_i$, it is possible to select this parameter so that the conditional probabilities given a sample $x_i$ have an uncertainty that resembles a $K$-sided die. \n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary><i>Proof for the perplexity of a uniform discret random variable </i></summary>\n",
    "\n",
    "   \\begin{aligned}\n",
    "    \\operatorname{Perplexity}(X) & :=2^{H(X)} \\\\\n",
    "    & =2^{\\frac{1}{K}-\\sum_{i=1}^K \\log _2 \\frac{1}{K}} \\\\\n",
    "    & =2^{-\\log _2 \\frac{1}{K}} \\\\\n",
    "    & =\\frac{1}{2^{\\log _2 \\frac{1}{K}}} \\\\\n",
    "    & =K\n",
    "    \\end{aligned}\n",
    "\n",
    "   From [8]\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### Binary search for $\\sigma_i$ <a class=\"anchor\" id=\"binary_search\"></a>\n",
    "\n",
    "Given a user-defined perplexity, the algorithm achieves a binary search (type of search algorithm) for every sample $x_i$ in the original space and finds a value of $\\sigma_i$ that matches the perplexity. To make the process faster we normally consider a user-defined number of nearest neighbor.\n",
    "\n",
    "<details>\n",
    "  <summary><i> Binary search? </i></summary>\n",
    "\n",
    "   A binary search is a type of search algorithm that 'finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.' [10]\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## t-Distributed Stochastic Neighbor Embedding <a class=\"anchor\" id=\"tsne\"></a>\n",
    "\n",
    "SNE gives interesting results although it has major draw backs such as :\n",
    "\n",
    "- Difficult to optimize cost function\n",
    "- 'crowding problem' (discussed later)\n",
    "\n",
    "t-SNE attempts to minimize these challenges by introducing a symmetric conditional probability and a introduces a Student-t distribution rather than a Gaussian function to compute the low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Symmetric SNE <a class=\"anchor\" id=\"sym_tsne\"></a>\n",
    "\n",
    "SNE minimizes the sum of the KL divergences between conditional probabilities $p_{i|j}$ and $q_{i|j}$\n",
    "\n",
    "$$\n",
    "C = \\sum_i KL(P_i||Q_i) = \\sum_i \\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}}\n",
    "$$\n",
    "\n",
    "Instead symmetric SNE minimizes a single KL divergence between a joint probability $P$ and $Q$.\n",
    "\n",
    "$$\n",
    "C = KL(P||Q) = \\sum_i \\sum_j p_{ij}\\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "Again $p_{ii}=0$ and $q_{ii}=0$ although $p_{ij}=p_{ji}$ and $q_{ij}=q_{ji}$.\n",
    "\n",
    "### Pairwise Similarites <a class=\"anchor\" id=\"paiwise_distance\"></a>\n",
    "\n",
    "$$\n",
    "\\text{high-dimensional : }\\quad\\quad p_{ij} = \\frac{p_{j|i}+p_{i|j}}{2n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{low-dimensional : }\\quad\\quad q_{ij} = \\frac{\\exp\\left( -||y_i - y_j||^2 \\right)}{\\sum_{k\\neq i}\\exp\\left( -||y_i - y_k||^2 \\right)}\n",
    "$$\n",
    "\n",
    "This increases the impact of outliers that normally would have little impact on the cost function by guaranteeing that $\\sum_j p_{ij} > 1/2n$ for all datapoints $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Crowding Problem <a class=\"anchor\" id=\"crowding\"></a>\n",
    "\n",
    "Sometimes it is impossible to create a faithful representation of a high-dimensional space simply by considering distance. We can imagine data embedded in a manifold that has a number of dimensional intrinsically higher than the umber of dimension used for the DR output. A simple example would be to represent the neighbors of points that form a equilateral triangle inside a one-dimensional space.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/triangle.svg\" alt=\"drawing\" width=\"300\"/>\n",
    "    <img src=\"Assets/line.svg\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "It does not matter how we rearrange the points inside the one-dimensional space we will always find some inconsistency with the original structure in terms of neighbors. \n",
    "\n",
    "Also inside the high-dimensional space there are many possibilities for points to exist moderately close to each other while being separated. The DR process will tend to place points with similar distances from clusters close to each other even if there is variety in the position inside the original space. Image points that follow a Gaussian distribution in 2 dimensions and need to be placed following a Gaussian in 1 dimension, points will have a limited amount of space to represent the complexity of the original space. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/2d_gaussian_points.svg\" alt=\"drawing\" width=\"300\"/>\n",
    "    <img src=\"Assets/1d_gaussian_points.svg\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mismatched Tails can Compensate for Mismatched Dimensionalities <a class=\"anchor\" id=\"mismatched_tails\"></a>\n",
    "\n",
    "To overcome the 'crowding problem' t-SNE uses a student t-distribution to represent the low-dimensional space which offers more space to the low-dimensional representation to separate data.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/t_vs_gauss.svg\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "The final low dimensional joint probability is defined as \n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k\\neq l} (1+||y_k - y_l||^2)^{-1}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Final Algorithm <a class=\"anchor\" id=\"final_alg\"></a>\n",
    "\n",
    "Exactly like SNE a gradient descent is used to optimize the cost function. The gradient becomes :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y_i} = \\sum_j(p_{ij} - q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}\n",
    "$$\n",
    "\n",
    "The final algorithm becomes :\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/algo.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's Implement t-SNE <a class=\"anchor\" id=\"implement\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization of the low-dimensional representation <a class=\"anchor\" id=\"init\"></a>\n",
    "\n",
    "$$\n",
    "Y = \\{y_1, y_2, ..., y_n\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_init(self):\n",
    "    \n",
    "    # Random initialization of low-dimensional representation Y\n",
    "    random_state = np.random.RandomState(42)\n",
    "    Y_embedded = 1e-4 * random_state.standard_normal( \n",
    "                size = (self.n_samples, self.n_components)\n",
    "            ).astype(np.float32)\n",
    "    Y = Y_embedded.ravel()\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix of euclidian distances <a class=\"anchor\" id=\"distance\"></a>\n",
    "\n",
    "We have a data matrix $X\\in{\\rm I\\!R}^{p\\times q}$ containing $p$ samples of size $q$.\n",
    "\n",
    "We want to build a distance matrix $D$ with elements :\n",
    "\n",
    "$$\n",
    "D_{ij} = ||x_i - x_j||^2 = \\left(\\sqrt{\\sum_{s=1}^q (x_{is} - x_{js})^2}\\right)^2 = \\sum_{s=i}^q (x_{is} - x_{js})^2\n",
    "$$\n",
    "\n",
    "<details>\n",
    "  <summary><i>Idea to compute D using matrices </i></summary>\n",
    "\n",
    "   Let's consider the initial data matrix $X$ with rows $x_i$.\n",
    "\n",
    "   $$\n",
    "    D_{ij} = (x_i - x_j)(x_i - x_j)^T = x_i \\cdot x_i - 2 x_i \\cdot x_j + x_j \\cdot x_j \n",
    "    $$\n",
    "    \n",
    "</details>\n",
    "\n",
    "Instead of evaluating the distances between every point, it is much more efficient to consider the $k$ nearest neighbors of a sample $x_i$ to compute the similarities. If the number of neighbors is adequate, the result is almost identical to the exact method. The gain in computation time can change the runtime from hours to seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def distances_knn(self, X : np.array):\n",
    "\n",
    "    # Find the nearest neighbors for every point\n",
    "    knn = NearestNeighbors(n_neighbors = self.n_neighbors, metric = \"euclidean\")\n",
    "    knn.fit(X)\n",
    "    distances_nn = knn.kneighbors_graph(mode = \"distance\")\n",
    "    del knn # Free memory\n",
    "    distances_nn.data **= 2\n",
    "\n",
    "    distances_nn.sort_indices()\n",
    "    n_samples = distances_nn.shape[0]\n",
    "    distances_data = distances_nn.data.reshape(n_samples, -1)\n",
    "    distances_data = distances_data.astype(np.float32, copy=False)\n",
    "\n",
    "    return distances_data, distances_nn.indices, distances_nn.indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint probabilities to represent Similarities (high-dimensional) <a class=\"anchor\" id=\"joint_prob\"></a>\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{Joint probability (high-dimensional) : }&\\quad\\quad p_{ij} = \\frac{p_{j|i}+p_{i|j}}{2n} \\\\\n",
    "    \\text{Conditional probability (high-dimensional) : }&\\quad\\quad  p_{j|i} = \\frac{\\exp\\left( -||x_i - x_j||^2 / 2\\sigma_i^2\\right)}{\\sum_{k\\neq i}\\exp\\left( -||x_i - x_k||^2 / 2\\sigma_i^2\\right)}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold._utils import _binary_search_perplexity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "MACHINE_EPSILON = np.finfo(np.double).eps\n",
    "\n",
    "def _joint_probabilities_nn(\n",
    "            self,\n",
    "            distances : np.array, \n",
    "            indices : np.array, \n",
    "            indptr : np.array\n",
    "        ) -> np.array:\n",
    "\n",
    "    # Binary search and conditional probability evaluation (in Cython)\n",
    "    conditional_P = _binary_search_perplexity(distances, self.perplexity, verbose = 0)\n",
    "\n",
    "    # Symmetrize the joint probability distribution using sparse operations\n",
    "    P = csr_matrix(\n",
    "        (conditional_P.ravel(), indices, indptr),\n",
    "        shape = (self.n_samples, self.n_samples),\n",
    "    )\n",
    "    P = P + P.T\n",
    "\n",
    "    # Normalize the joint probability distribution\n",
    "    sum_P = np.maximum(P.sum(), self.MACHINE_EPSILON)\n",
    "    P /= sum_P\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the KL divergence <a class=\"anchor\" id=\"gradient\"></a>\n",
    "\n",
    "Considering the joint probabilities based on the student-t distribution\n",
    "\n",
    "$$\n",
    "\\text{Joint probability (low-dimensional) :}\\quad\\quad q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k\\neq l} (1+||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "The KL divergence is never explicitly evaluated since we only evaluate the gradient descent. It is the gradient of the KL divergence that matters. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y_i} = \\sum_j(p_{ij} - q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import _barnes_hut_tsne\n",
    "\n",
    "def _kl_divergence_bh(\n",
    "        self,\n",
    "        Y : np.array,\n",
    "        P # compressed sparse matrix\n",
    "    ):\n",
    "\n",
    "    Y = Y.astype(np.float32, copy=False)\n",
    "    Y_embedded = Y.reshape(self.n_samples, self.n_components)\n",
    "\n",
    "    val_P = P.data.astype(np.float32, copy=False)\n",
    "    neighbors = P.indices.astype(np.int64, copy=False)\n",
    "    indptr = P.indptr.astype(np.int64, copy=False)\n",
    "\n",
    "    grad = np.zeros(Y_embedded.shape, dtype=np.float32)\n",
    "    # Gradient written in Cython\n",
    "    error = _barnes_hut_tsne.gradient(\n",
    "        val_P = val_P,\n",
    "        pos_output = Y_embedded,\n",
    "        neighbors = neighbors,\n",
    "        indptr = indptr,\n",
    "        forces = grad,\n",
    "        theta = 0.5,\n",
    "        n_dimensions = self.n_components,\n",
    "        verbose = 0,\n",
    "        dof = self.degrees_of_freedom,\n",
    "        num_threads = self.num_threads,\n",
    "    )\n",
    "\n",
    "    c = 2.0 * (self.degrees_of_freedom + 1.0) / self.degrees_of_freedom\n",
    "    grad = grad.ravel()\n",
    "    grad *= c\n",
    "\n",
    "    return error, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent <a class=\"anchor\" id=\"gradient_descent_code\"></a>\n",
    "\n",
    "The gradient descent is done through an iterative process where the following equation is evaluated :  \n",
    "\n",
    "$$\n",
    "Y^{(t)} = Y^{(t-1)} + \\eta \\frac{\\partial C}{\\partial y_i} +\\alpha(t) (Y^{(t-1)} - Y^{(t-2)})\n",
    "$$\n",
    "\n",
    "- $Y^{(t)}$ : Solution at iteration $t$.\n",
    "- $\\eta$ : Learning rate\n",
    "- $\\alpha(t)$ : Momentum at iteration $t$\n",
    "- $\\frac{\\partial C}{\\partial y_i}$ : Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(\n",
    "        self,\n",
    "        Y0 : np.array,\n",
    "        P , # compressed sparse matrix\n",
    "        momentum : float = 0.8\n",
    "    ):\n",
    "\n",
    "    Y = Y0.copy().ravel()\n",
    "    update = np.zeros_like(Y)\n",
    "    gains = np.ones_like(Y)\n",
    "    error = np.finfo(float).max\n",
    "    best_error = np.finfo(float).max\n",
    "    best_iter = i = self.it\n",
    "\n",
    "    for i in range(self.it, self.n_iter):\n",
    "\n",
    "        error, grad = self._kl_divergence_bh(Y = Y, P = P)\n",
    "\n",
    "        inc = update * grad < 0.0\n",
    "        dec = np.invert(inc)\n",
    "        gains[inc] += 0.2\n",
    "        gains[dec] *= 0.8\n",
    "        np.clip(gains, 0.01, np.inf, out=gains) # min_gain = 0.01\n",
    "        grad *= gains\n",
    "        update = momentum * update - self.learning_rate_ * grad\n",
    "        Y += update\n",
    "\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "        # Check progress of gradient descent\n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_iter = i\n",
    "        elif i - best_iter > 300: # n_iter_without_progress\n",
    "            break\n",
    "        if grad_norm <= 1e-7: # min_grad_norm\n",
    "            break\n",
    "\n",
    "    return Y, error, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile everything in one class <a class=\"anchor\" id=\"compile\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(self, X : np.array) -> np.array:\n",
    "\n",
    "    # Initialize optimization parameters\n",
    "    self.initialize_params(X)\n",
    "\n",
    "    # Compute the distances\n",
    "    distances_nn, indices, indptr = self.distances_knn(X = X)\n",
    "    \n",
    "    # compute the joint probability distribution for the input space\n",
    "    P = self._joint_probabilities_nn(\n",
    "                                distances = distances_nn, \n",
    "                                indices = indices,\n",
    "                                indptr = indptr\n",
    "                            )\n",
    "\n",
    "    # Random initialization of low-dimensional representation Y\n",
    "    Y = self.random_init()\n",
    "\n",
    "    # Learning schedule (part 1): do 250 iteration with lower momentum but\n",
    "    # higher learning rate controlled via the early exaggeration parameter\n",
    "    P *= self.early_exaggeration\n",
    "    Y, self.kl_divergence_, it = self._gradient_descent(\n",
    "                                        Y0 = Y, \n",
    "                                        P = P,\n",
    "                                        momentum = 0.5\n",
    "                                )\n",
    "\n",
    "    # Learning schedule (part 2): disable early exaggeration and finish\n",
    "    # optimization with a higher momentum at 0.8\n",
    "    P /= self.early_exaggeration\n",
    "    remaining = self.n_iter - self._EXPLORATION_N_ITER\n",
    "    if self.it < self._EXPLORATION_N_ITER or remaining > 0:\n",
    "        self.it = self.it + 1\n",
    "        Y, self.kl_divergence_, self.it = self._gradient_descent(\n",
    "                                        Y0 = Y, \n",
    "                                        P = P, \n",
    "                                        momentum = 0.8\n",
    "                                    )\n",
    "\n",
    "    return Y.reshape(self.n_samples, self.n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete code <a class=\"anchor\" id=\"complete_code\"></a>\n",
    "\n",
    "The previous functions were meant to be used in a single class. This is why some functions may not run on their own and why the word `self.` is in front of so many variables (`self` represents the instance of the class). For the sake of demonstration and simplicity, many variables are not discussed and it is recommended to use the original class from the Scikit-learn library when implementing t-SNE. The goal of this simplified version is to give a general understanding of how t-SNE is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold._utils import _binary_search_perplexity\n",
    "from sklearn.manifold import _barnes_hut_tsne\n",
    "\n",
    "# Optimized processes (not discussed)\n",
    "from sklearn.utils._openmp_helpers import _openmp_effective_n_threads #Number of threads\n",
    "\n",
    "class TSNE():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components = 2,\n",
    "        perplexity = 30.0\n",
    "    ):\n",
    "        # Control the number of exploration iterations with early_exaggeration on\n",
    "        self._EXPLORATION_N_ITER = 250\n",
    "        # Control the number of iterations between progress checks\n",
    "        self._N_ITER_CHECK = 50\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.perplexity = perplexity\n",
    "        self.early_exaggeration = 12.0\n",
    "        self.n_iter = 1000 # number of iteration\n",
    "        self.it = 0 # Current iteration number\n",
    "\n",
    "        self.n_samples = None\n",
    "        self.learning_rate_ = None\n",
    "        self.n_neighbors = None\n",
    "        self.degrees_of_freedom = None\n",
    "        self.num_threads = None\n",
    "\n",
    "    \n",
    "    def initialize_params(self, X):\n",
    "\n",
    "        # Define number of samples\n",
    "        self.n_samples = X.shape[0]\n",
    "        # Always learning_rate_ = 'auto'\n",
    "        self.learning_rate_ = np.maximum(self.n_samples / self.early_exaggeration / 4, 50)\n",
    "        # Compute the number of nearest neighbors to find.\n",
    "        self.n_neighbors = min(self.n_samples - 1, int(3.0 * self.perplexity + 1))\n",
    "        # Degree of freedom in gradient descent\n",
    "        self.degrees_of_freedom = max(self.n_components - 1, 1)\n",
    "        # Define the number of available threads\n",
    "        self.num_threads = _openmp_effective_n_threads()\n",
    "        # Numerical precision\n",
    "        self.MACHINE_EPSILON = np.finfo(np.double).eps\n",
    "\n",
    "\n",
    "    def random_init(self):\n",
    "        \n",
    "        # Random initialization of low-dimensional representation Y\n",
    "        random_state = np.random.RandomState(42)\n",
    "        X_embedded = 1e-4 * random_state.standard_normal( \n",
    "                    size = (self.n_samples, self.n_components)\n",
    "                ).astype(np.float32)\n",
    "        params = X_embedded.ravel()\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "    def distances_knn(self, X : np.array):\n",
    "\n",
    "        # Find the nearest neighbors for every point\n",
    "        knn = NearestNeighbors(n_neighbors = self.n_neighbors, metric = \"euclidean\")\n",
    "        knn.fit(X)\n",
    "        distances_nn = knn.kneighbors_graph(mode = \"distance\")\n",
    "        del knn # Free memory\n",
    "        distances_nn.data **= 2\n",
    "\n",
    "        distances_nn.sort_indices()\n",
    "        n_samples = distances_nn.shape[0]\n",
    "        distances_data = distances_nn.data.reshape(n_samples, -1)\n",
    "        distances_data = distances_data.astype(np.float32, copy=False)\n",
    "\n",
    "        return distances_data, distances_nn.indices, distances_nn.indptr\n",
    "    \n",
    "\n",
    "    def _joint_probabilities_nn(\n",
    "            self,\n",
    "            distances : np.array, \n",
    "            indices : np.array, \n",
    "            indptr : np.array\n",
    "        ) -> np.array:\n",
    "\n",
    "        # Binary search and conditional probability evaluation (in Cython)\n",
    "        conditional_P = _binary_search_perplexity(distances, self.perplexity, verbose = 0)\n",
    "\n",
    "        # Symmetrize the joint probability distribution using sparse operations\n",
    "        P = csr_matrix(\n",
    "            (conditional_P.ravel(), indices, indptr),\n",
    "            shape = (self.n_samples, self.n_samples),\n",
    "        )\n",
    "        P = P + P.T\n",
    "\n",
    "        # Normalize the joint probability distribution\n",
    "        sum_P = np.maximum(P.sum(), self.MACHINE_EPSILON)\n",
    "        P /= sum_P\n",
    "\n",
    "        return P\n",
    "\n",
    "\n",
    "\n",
    "    def _kl_divergence_bh(\n",
    "        self,\n",
    "        Y : np.array,\n",
    "        P # compressed sparse matrix\n",
    "    ):\n",
    "\n",
    "        Y = Y.astype(np.float32, copy=False)\n",
    "        Y_embedded = Y.reshape(self.n_samples, self.n_components)\n",
    "\n",
    "        val_P = P.data.astype(np.float32, copy=False)\n",
    "        neighbors = P.indices.astype(np.int64, copy=False)\n",
    "        indptr = P.indptr.astype(np.int64, copy=False)\n",
    "\n",
    "        grad = np.zeros(Y_embedded.shape, dtype=np.float32)\n",
    "        error = _barnes_hut_tsne.gradient(\n",
    "            val_P = val_P,\n",
    "            pos_output = Y_embedded,\n",
    "            neighbors = neighbors,\n",
    "            indptr = indptr,\n",
    "            forces = grad,\n",
    "            theta = 0.5,\n",
    "            n_dimensions = self.n_components,\n",
    "            verbose = 0,\n",
    "            dof = self.degrees_of_freedom,\n",
    "            num_threads = self.num_threads,\n",
    "        )\n",
    "\n",
    "        c = 2.0 * (self.degrees_of_freedom + 1.0) / self.degrees_of_freedom\n",
    "        grad = grad.ravel()\n",
    "        grad *= c\n",
    "\n",
    "        return error, grad\n",
    "\n",
    "\n",
    "\n",
    "    def _gradient_descent(\n",
    "        self,\n",
    "        Y0 : np.array,\n",
    "        P , # compressed sparse matrix\n",
    "        momentum : float = 0.8\n",
    "    ):\n",
    "\n",
    "        Y = Y0.copy().ravel()\n",
    "        update = np.zeros_like(Y)\n",
    "        gains = np.ones_like(Y)\n",
    "        error = np.finfo(float).max\n",
    "        best_error = np.finfo(float).max\n",
    "        best_iter = i = self.it\n",
    "\n",
    "        for i in range(self.it, self.n_iter):\n",
    "\n",
    "            error, grad = self._kl_divergence_bh(Y = Y, P = P)\n",
    "\n",
    "            inc = update * grad < 0.0\n",
    "            dec = np.invert(inc)\n",
    "            gains[inc] += 0.2\n",
    "            gains[dec] *= 0.8\n",
    "            np.clip(gains, 0.01, np.inf, out=gains) # min_gain = 0.01\n",
    "            grad *= gains\n",
    "            update = momentum * update - self.learning_rate_ * grad\n",
    "            Y += update\n",
    "\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "            # Check progress of gradient descent\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_iter = i\n",
    "            elif i - best_iter > 300: # n_iter_without_progress\n",
    "                break\n",
    "            if grad_norm <= 1e-7: # min_grad_norm\n",
    "                break\n",
    "\n",
    "        return Y, error, i\n",
    "\n",
    "\n",
    "    def fit_transform(self, X : np.array) -> np.array:\n",
    "\n",
    "        # Initialize optimization parameters\n",
    "        self.initialize_params(X)\n",
    "\n",
    "        # Compute the distances\n",
    "        distances_nn, indices, indptr = self.distances_knn(X = X)\n",
    "        \n",
    "        # compute the joint probability distribution for the input space\n",
    "        P = self._joint_probabilities_nn(\n",
    "                                    distances = distances_nn, \n",
    "                                    indices = indices,\n",
    "                                    indptr = indptr\n",
    "                                )\n",
    "\n",
    "        # Random initialization of low-dimensional representation Y\n",
    "        Y = self.random_init()\n",
    "\n",
    "        # Learning schedule (part 1): do 250 iteration with lower momentum but\n",
    "        # higher learning rate controlled via the early exaggeration parameter\n",
    "        P *= self.early_exaggeration\n",
    "        Y, self.kl_divergence_, it = self._gradient_descent(\n",
    "                                            Y0 = Y, \n",
    "                                            P = P,\n",
    "                                            momentum = 0.5\n",
    "                                    )\n",
    "\n",
    "        # Learning schedule (part 2): disable early exaggeration and finish\n",
    "        # optimization with a higher momentum at 0.8\n",
    "        P /= self.early_exaggeration\n",
    "        remaining = self.n_iter - self._EXPLORATION_N_ITER\n",
    "        if self.it < self._EXPLORATION_N_ITER or remaining > 0:\n",
    "            self.it = self.it + 1\n",
    "            Y, self.kl_divergence_, self.it = self._gradient_descent(\n",
    "                                            Y0 = Y, \n",
    "                                            P = P, \n",
    "                                            momentum = 0.8\n",
    "                                        )\n",
    "\n",
    "        return Y.reshape(self.n_samples, self.n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the described implementation <a class=\"anchor\" id=\"test_implementation\"></a>\n",
    "\n",
    "Let's test the implementation on a dataset that contains signals from transition edge sensor (TES) which are photon number resolving detectors. The signals in the dataset follow a Poisson distribution with an average of $\\bar{n}=2.263$ photons. The signals and the expected photon number distribution is the following :\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/signals.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/expected_distribution.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "The dataset contains 10 240 samples of size 350 and we will reduce the dimension of the samples in a 2 dimensional space. \n",
    "\n",
    "To run t-SNE we compile all the code described previously in a file `reduced_sklearn_tsne.py` and execute the TSNE class using the following code. We first use a perplexity of 10 considering or dataset contains 10 classes (10 possible photon numbers). This value is not ideal since it should be associated to a dataset with uniform class probability but it gives use a place to start looking.\n",
    "\n",
    "```python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from Dataset import dataset_TES\n",
    "from reduced_sklearn_tsne import TSNE\n",
    "\n",
    "data = dataset_TES()\n",
    "\n",
    "perplexity = 10\n",
    "model = TSNE(n_components = 2, perplexity=perplexity)\n",
    "reduced = model.fit_transform(data)\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8\"):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.scatter(reduced[:,0], reduced[:,1], s=1)\n",
    "    plt.xlabel(r'$s_1$')\n",
    "    plt.ylabel(r'$s_2$')\n",
    "    plt.title(f'Perplexity : {perplexity}', fontsize = 20)\n",
    "    plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code runs in a few seconds on a good computer ($\\sim$ 5 seconds) and gives the following output :\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/tsne_output_10.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "We can clearly identify 9 clusters that are associated to the expected photon number distribution!\n",
    "\n",
    "### Now lets look how perplexity changes our output\n",
    "\n",
    "If we run the same code for multiple values of perplexity we get the following :\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/tsne_output_2.png\" alt=\"drawing\" width=\"250\"/>\n",
    "    <img src=\"Assets/tsne_output_5.png\" alt=\"drawing\" width=\"250\"/>\n",
    "    <img src=\"Assets/tsne_output_10.png\" alt=\"drawing\" width=\"250\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/tsne_output_20.png\" alt=\"drawing\" width=\"250\"/>\n",
    "    <img src=\"Assets/tsne_output_50.png\" alt=\"drawing\" width=\"250\"/>\n",
    "    <img src=\"Assets/tsne_output_100.png\" alt=\"drawing\" width=\"250\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Assets/tsne_output_1000.png\" alt=\"drawing\" width=\"250\"/>\n",
    "    <img src=\"Assets/tsne_output_10000.png\" alt=\"drawing\" width=\"250\"/>\n",
    "</p>\n",
    "\n",
    "The runtime increases with perplexity, for a value of 2 it took 4 seconds and for a perplexity of 1000 it took 77 seconds.\n",
    "\n",
    "We can see that increasing the perplexity in this case makes the clusters denser up to a certain point where the output becomes difficult to understand. Playing around with the different parameters can give information about high-dimensional data but this is the subject of another discussion. If you still want more information, the following article does a great job at discussing the impact of t-SNE parameters :\n",
    "\n",
    ">[11] M. Wattenberg, F. Viégas, and I. Johnson, ‘How to Use t-SNE Effectively’, Distill, vol. 1, no. 10, p. e2, Oct. 2016, doi: 10.23915/distill.00002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"references\"></a>\n",
    "\n",
    "\n",
    "[1] L. Van der Maaten and G. Hinton, ‘Visualizing data using t-SNE.’, Journal of machine learning research, vol. 9, no. 11, 2008, Accessed: Apr. 03, 2024. [Online]. Available: https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl\n",
    "\n",
    "[2] L. Schoneveld, ‘In Raw Numpy: t-SNE’, nlml. Accessed: Apr. 05, 2024. [Online]. Available: https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/\n",
    "\n",
    "[3] ‘t-SNE clearly explained - Blog by Kemal Erdem’, t-SNE clearly explained - Blog by Kemal Erdem. Accessed: Apr. 05, 2024. [Online]. Available: https://erdem.pl/2020/04/t-sne-clearly-explained\n",
    "\n",
    "[4] ‘sklearn.manifold.TSNE’, scikit-learn. Accessed: Apr. 05, 2024. [Online]. Available: https://scikit-learn/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "[5] ‘Kullback–Leibler divergence’, Wikipedia. Mar. 15, 2024. Accessed: Apr. 05, 2024. [Online]. Available: https://en.wikipedia.org/w/index.php?title=Kullback%E2%80%93Leibler_divergence&oldid=1213814981\n",
    "\n",
    "[6] ‘Perplexity: a more intuitive measure of uncertainty than entropy’, Matthew N. Bernstein. Accessed: Apr. 06, 2024. [Online]. Available: https://mbernste.github.io/posts/perplexity/\n",
    "\n",
    "[7] ‘Entropy of the Gaussian’. Accessed: Apr. 06, 2024. [Online]. Available: https://gregorygundersen.com/blog/2020/09/01/gaussian-entropy/\n",
    "\n",
    "[8] ‘Perplexity’, Wikipedia. Mar. 11, 2024. Accessed: Apr. 06, 2024. [Online]. Available: https://en.wikipedia.org/w/index.php?title=Perplexity&oldid=1213187763\n",
    "\n",
    "[9] S. M. Barnett, Quantum information. in Oxford master series in physics Atomic, optical, and laser physics, no. 16. New York, N.Y.: Oxford University Press, 2009.\n",
    "\n",
    "[10] ‘Binary search algorithm’, Wikipedia. Mar. 15, 2024. Accessed: Apr. 07, 2024. [Online]. Available: https://en.wikipedia.org/w/index.php?title=Binary_search_algorithm&oldid=1213923550\n",
    "\n",
    "[11] M. Wattenberg, F. Viégas, and I. Johnson, ‘How to Use t-SNE Effectively’, Distill, vol. 1, no. 10, p. e2, Oct. 2016, doi: 10.23915/distill.00002.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
